{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    from tqdm.autonotebook import tqdm\n",
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "import pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# Set up the Pinecone vector database\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = \"text-embedding-ada-002\"\n",
    "embed = OpenAIEmbeddings(model=model_name, openai_api_key=OPENAI_API_KEY)\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = \"us-east-1-aws\"\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name) #this uploads embeddings and other data to Pinecone\n",
    "index_name = \"doc-start\"\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_data_extractor(file_name):\n",
    "    case_no = file_name.split('/')[-1].split('.')[0]\n",
    "    district = file_name.split('/')[-3].split('_')[-1]\n",
    "    year = int(file_name.split('/')[-2])\n",
    "    url = f'https://www.supremecourt.ohio.gov/rod/docs/pdf/{district}/{year}/{case_no}.pdf'\n",
    "    return case_no, district, year, url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECTING RANDOM OPINION FILES FOR TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def select_random_files_from_year_subdirs(root_directory, num_files=100):\n",
    "    \"\"\"\n",
    "    Selects a specified number of random files from year subdirectories under each district directory\n",
    "    within the given root directory. Assumes district directories are named 'District_1' through 'District_12'\n",
    "    and year subdirectories are named with years.\n",
    "\n",
    "    Parameters:\n",
    "    root_directory (str): Path to the root directory.\n",
    "    num_files (int): Number of files to select. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of full paths of selected files.\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "\n",
    "    # Define district directories\n",
    "    district_dirs = [f'District_{i}' for i in range(1, 13)]\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for district in district_dirs:\n",
    "        district_path = os.path.join(root_directory, district)\n",
    "        if os.path.exists(district_path):\n",
    "            for year in os.listdir(district_path):\n",
    "                year_path = os.path.join(district_path, year)\n",
    "                if os.path.isdir(year_path) and year.isdigit():\n",
    "                    for file in os.listdir(year_path):\n",
    "                        file_path = os.path.join(year_path, file)\n",
    "                        if os.path.isfile(file_path):\n",
    "                            all_files.append(file_path)\n",
    "\n",
    "    # Select num_files random files from the list\n",
    "    selected_files = random.sample(all_files, min(len(all_files), num_files))\n",
    "\n",
    "    return selected_files\n",
    "\n",
    "# Example usage\n",
    "# Replace 'path/to/ohio_case_scrape' with the actual path to your 'ohio_case_scrape' directory\n",
    "root_directory = '/Users/deantaylor/ohio_case_scrape'\n",
    "file_lst = select_random_files_from_year_subdirs(root_directory)\n",
    "# print(random_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_lst:\n",
    "    case_no, district, year, url = meta_data_extractor(file_name)\n",
    "    # file_name = '/Users/deantaylor/ohio_case_scrape/District_12/2005/2005-Ohio-5048.pdf'\n",
    "    loader = PyPDFLoader(file_name)\n",
    "    data = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    "    )  #required for loading into the embeddings model because of the limited context window\n",
    "    texts = text_splitter.split_documents(data)\n",
    "    for t in texts:  #this cleans up the text and adds metadata while removing the reference to the source file on my system which is not needed\n",
    "        #replace the \\n characters with spaces\n",
    "        t.page_content = t.page_content.replace('\\n', ' ')\n",
    "        t.metadata[\"case_no\"] = case_no\n",
    "        t.metadata[\"district\"] = district\n",
    "        t.metadata[\"year\"] = int(year)\n",
    "        t.metadata[\"url\"] = url\n",
    "        t.metadata.pop('source')\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) #creates the object to make the embeddings, does not hold any data itself\n",
    "    test_cone = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = '/Users/deantaylor/ohio_case_scrape/District_12/2005/2005-Ohio-5048.pdf'\n",
    "# loader = PyPDFLoader(file_name)\n",
    "# data = loader.load()\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "# separator = \"\\n\",\n",
    "# chunk_size = 2000,\n",
    "# chunk_overlap  = 200,\n",
    "# )\n",
    "# texts = text_splitter.split_documents(data)\n",
    "# print(f'total length of texts is {len(texts)}')\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_no, district, year, url = meta_data_extractor(file_name)\n",
    "# print(f'case no: {case_no}')\n",
    "# print(F'appellate district: {district}')\n",
    "# print(f'year: {year}')\n",
    "# print(f'url: {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in texts:\n",
    "#     t.metadata[\"case_no\"] = case_no\n",
    "#     t.metadata[\"district\"] = district\n",
    "#     t.metadata[\"year\"] = year\n",
    "#     t.metadata[\"url\"] = url\n",
    "#     t.metadata.pop('source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) #creates the object to make the embeddings, does not hold any data itself\n",
    "# test_cone = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE ABOVE IS WORKING AND PRODUCING MODIFIED META DATA IN THE PINECONE DB ALONG WITH VECTORS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BELOW WORKING TO ADD DOCUMENTS IN BULK WITH META DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_name in file_lst:\n",
    "#     loader = PyPDFLoader(file_name)\n",
    "#     data = loader.load()\n",
    "#     text_splitter = CharacterTextSplitter(\n",
    "#     separator = \"\\n\",\n",
    "#     chunk_size = 2000,\n",
    "#     chunk_overlap  = 200,\n",
    "#     )\n",
    "#     texts = text_splitter.split_documents(data)\n",
    "#     print(f'file_name: {file_name}, texts length: {len(texts)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEARCHING THE DB FOR INFORMATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "query = \"What is the name of the defendant in case number 2005-Ohio-287?\"\n",
    "docs = test_cone.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERY WITHOUT FILTERING ON META DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "queries = [[random.random() for i in range(1536)]]\n",
    "index.query(queries, top_k=10, include_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE WILL QUERY THE DB WITH REFERENCE TO META DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS FILTERS FOR CASES NOT IN DISTRICT 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.query(\n",
    "    queries=queries, \n",
    "    top_k=10, \n",
    "    include_metadata=True,\n",
    "    filter={\"district\": {\"$ne\":\"12\"}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GETTING ALL ITEMS IN DB FOR A SPECIFIC CASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_no = \"2009-Ohio-6689\"\n",
    "index.query(\n",
    "    queries=queries, \n",
    "    top_k=10, \n",
    "    include_metadata=True,\n",
    "    filter={\"case_no\": {\"$eq\":case_no}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].page_content\n",
    "texts[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and prepare metadata\n",
    "data_to_upload = []\n",
    "for t in texts:\n",
    "    embedding = embeddings.embed(t.page_content)\n",
    "    metadata = {\n",
    "        'url': \"the url\",\n",
    "        'case_no': \"the case no\",\n",
    "        # include other metadata fields as needed\n",
    "    }\n",
    "    data_to_upload.append((t.id, embedding, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) #creates the object to make the embeddings, does not hold any data itselfdoc_search = \n",
    "upsert_response = index.upsert(\n",
    "   vectors=[t.page_content for t in texts]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name) #this uploads embeddings and other data to Pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAB RANDOM TEN FILES FROM A DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def select_random_files(directory, num_files=10):\n",
    "    \"\"\"\n",
    "    Selects a specified number of random files from a given directory and returns their full paths.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory.\n",
    "    num_files (int): Number of files to select. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of full paths of selected files.\n",
    "    \"\"\"\n",
    "    # Get a list of files in the directory with their full paths\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "    # Select num_files random files from the list\n",
    "    selected_files = random.sample(files, min(len(files), num_files))\n",
    "\n",
    "    return selected_files\n",
    "\n",
    "file_lst = select_random_files(\"District_8/2010\")\n",
    "# file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedder(file_name):\n",
    "    case_no, district, year, url = meta_data_extractor(file_name)\n",
    "    print(f'case no: {case_no}')\n",
    "    print(F'appellate district: {district}')\n",
    "    print(f'year: {year}')\n",
    "    print(f'url: {url}')\n",
    "    loader = PyPDFLoader(file_name)\n",
    "    data = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    "    )\n",
    "    texts = text_splitter.split_documents(data)\n",
    "    # for t in texts:\n",
    "    #     t.page_content = t.page_content.replace(\"\\n\", \" \")\n",
    "    #     t.metadata[\"url\"] = url\n",
    "    #     t.metadata[\"case_no\"] = case_no\n",
    "    #     t.metadata[\"district\"] = district\n",
    "    #     t.metadata[\"year\"] = year\n",
    "    #     #remove the source key from the t.metadata\n",
    "    #     t.metadata.pop('source')\n",
    "    # print(texts[0].metadata)\n",
    "    print(texts[0])\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) #creates the object to make the embeddings, does not hold any data itself\n",
    "    Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name) #this uploads embeddings and other data to Pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder('/Users/deantaylor/ohio_case_scrape/District_8/2005/2005-Ohio-24.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_lst:\n",
    "    embedder(file_name)\n",
    "    print(f'uploaded {file_name} to pinecone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFLoader(file_name)\n",
    "# data = loader.load()\n",
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "# print (f'You have {len(data)} document(s) in your data')\n",
    "# print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
    "# print (f'Here is a sample: {data[0].page_content[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/Users/deantaylor/ohio_case_scrape/District_2/2010/2010-Ohio-3652.pdf\"\n",
    "case_no = file_name.split('/')[-1].split('.')[0]\n",
    "print(f'case no: {case_no}')\n",
    "#get the district number which is the number at the end of this 'District_12\"\n",
    "district = file_name.split('/')[-3].split('_')[-1]\n",
    "print(F'appellate district: {district}')\n",
    "year = file_name.split('/')[-2]\n",
    "print(f'year: {year}')\n",
    "#now build the url for this case number\n",
    "#format is https://www.supremecourt.ohio.gov/rod/docs/pdf/1/2023/2023-Ohio-4551.pdf\n",
    "url = f'https://www.supremecourt.ohio.gov/rod/docs/pdf/{district}/{year}/{case_no}.pdf'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"{Â¶\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through all the t.page_content in the texts and remove the \\n charazxter and replace with space\n",
    "for t in texts:\n",
    "    t.page_content = t.page_content.replace(\"\\n\", \" \")\n",
    "texts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = ''\n",
    "test_text = texts[0]\n",
    "# print(test_text)\n",
    "# print(test_text.page_content)\n",
    "# print(test_text.metadata)\n",
    "test_text.metadata[\"url\"] = url\n",
    "test_text.metadata[\"case_no\"] = case_no\n",
    "test_text.metadata[\"district\"] = district\n",
    "test_text.metadata[\"year\"] = year\n",
    "#remove the source key from the test_text.metadata\n",
    "test_text.metadata.pop('source')\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # print(test_text.metadata)\n",
    "# # # Assuming 'doc' is an instance of langchain_core.metadatas.base.Document\n",
    "# page_content = test_text.page_content\n",
    "# metadata = test_text.metadata\n",
    "# print(f'metadata from the document is {metadata}')\n",
    "# metadata\n",
    "# # print(test_text[\"metadata\"])\n",
    "# url = \"https://wwww.courtlistener.com\"\n",
    "# district = \"District 12\"\n",
    "# case_no = \"2005-Ohio-327\"\n",
    "\n",
    "# for test_text.metadata in test_text:\n",
    "#     print(metadata)\n",
    "\n",
    "    # metadata[\"district\"] = district\n",
    "    # metadata[\"case_no\"] = case_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New key-value pairs to add\n",
    "url = \"http://example.com/case/2005-Ohio-327\"\n",
    "district = \"Twelfth Appellate District\"\n",
    "case_no = \"2005-Ohio-327\"\n",
    "\n",
    "\n",
    "for text in texts:\n",
    "    t.page_content = t.page_content.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "# Adding the key-value pairs to the metadata\n",
    "for document in data:\n",
    "    document[\"metadata\"][\"url\"] = url\n",
    "    document[\"metadata\"][\"district\"] = district\n",
    "    document[\"metadata\"][\"case_no\"] = case_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) #creates the object to make the embeddings, does not hold any data itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metademata = {\"url\": \"opinionurl.com\", \"case_name\": \"Case Name\", \"year\": \"2005\", \"court\": \"Ohio Supreme Court\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst_texts = [t.page_content for t in texts[:1]]\n",
    "test_lst_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")\n",
    "\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name) #this uploads embeddings and other data to Pinecone\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name) #this uploads embeddings and other data to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERY THE DOCUMENT THAT WAS JUST VECTORIZED IN PINECONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "query = \"What is the name of the defendant in this case?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1= \"What are the rules or statutes involved in this case?\"\n",
    "docs = docsearch.similarity_search(query1)\n",
    "chain.run(input_documents=docs, question=query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1= \"Summarize all of the facts of this case like a lawyer analyzing the case and do it in a list of sentences.\"\n",
    "docs = docsearch.similarity_search(query1)\n",
    "chain.run(input_documents=docs, question=query1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
