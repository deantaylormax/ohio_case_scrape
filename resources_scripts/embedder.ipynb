{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "import pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone \n",
    "loader = PyPDFLoader(\"/Users/deantaylor/ohio_case_scrape/District_4/2013/2013-Ohio-3861.pdf\")\n",
    "environment = 'us-east-1-aws'\n",
    "# Initialize OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=environment)\n",
    "# Define the index name\n",
    "index_name = \"doc-chat\"\n",
    "# Create the index if it doesn't exist\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=1536)\n",
    "# Instantiate the index\n",
    "index = pinecone.Index(index_name)\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Replace consecutive spaces, newlines and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    # create a loader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    # load your data\n",
    "    data = loader.load()\n",
    "    # Split your data up into smaller documents with Chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(data)\n",
    "    # Convert Document objects into strings\n",
    "    texts = [str(doc) for doc in documents]\n",
    "    return texts\n",
    "\n",
    "# Define a function to create embeddings\n",
    "def create_embeddings(texts):\n",
    "    embeddings_list = []\n",
    "    for text in texts:\n",
    "        # res = openai.Embedding.create(input=[text], engine=MODEL)\n",
    "        res = client.embeddings.create(input=text, model=MODEL).data[0].embedding #this gets just the embedding array\n",
    "        # res = client.embeddings.create(input=text, model=MODEL)[\"data\"][0][\"embedding\"]\n",
    "        embeddings_list.append(res)\n",
    "    return embeddings_list\n",
    "\n",
    "# Define a function to upsert embeddings to Pinecone\n",
    "def upsert_embeddings_to_pinecone(index, embeddings, ids):\n",
    "    index.upsert(vectors=[(id, embedding) for id, embedding in zip(ids, embeddings)])\n",
    "\n",
    "# Process a PDF and create embeddings\n",
    "file_path = \"/Users/deantaylor/ohio_case_scrape/District_4/2013/2013-Ohio-3861.pdf\"  # Replace with your actual file path\n",
    "texts = process_pdf(file_path)\n",
    "embeddings = create_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embeddings)\n",
    "upsert_embeddings_to_pinecone(index, embeddings, [file_path])\n",
    "# embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_qa_chain\n\u001b[0;32m----> 2\u001b[0m docsearch \u001b[38;5;241m=\u001b[39m \u001b[43mPinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaht is the summary of this document\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/ohio_case_scrape/.venv/lib/python3.11/site-packages/langchain_core/vectorstores.py:508\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[VST],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     texts \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    509\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/ohio_case_scrape/.venv/lib/python3.11/site-packages/langchain_core/vectorstores.py:508\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[VST],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    509\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "docsearch = Pinecone.from_documents(texts[0], embeddings, index_name=index_name)\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "query = \"waht is the summary of this document\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query7 = \"Was this a criminal or civil case?\"\n",
    "resp7 = chain.run(input_documents=docs, question=query7)\n",
    "print(f\"{query7}: {resp7}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 32\u001b[0m\n\u001b[1;32m     24\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m     25\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m     26\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     27\u001b[0m     separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m text \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/deantaylor/ohio_case_scrape/District_4/2013/2013-Ohio-3416.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     33\u001b[0m chunks\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# PDF path and metadata\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# pdf_path = \"/Users/deantaylor/ohio_case_scrape/District_4/2013/2013-Ohio-3416.pdf\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# url = \"https://example.com/pdf_file.pdf\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# docs = text_splitter.split_documents(text_content)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# print(docs)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "import os\n",
    "import dotenv\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "# Connect to Pinecone\n",
    "index_name = \"doc-chat\"\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in range(len(reader.pages)):\n",
    "            text += reader.pages[page].extract_text()\n",
    "        return text\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "text = extract_text_from_pdf(\"/Users/deantaylor/ohio_case_scrape/District_4/2013/2013-Ohio-3416.pdf\")\n",
    "\n",
    "chunks = text_splitter.split_text(text[6]['text'])[:3]\n",
    "chunks\n",
    "# PDF path and metadata\n",
    "# pdf_path = \"/Users/deantaylor/ohio_case_scrape/District_4/2013/2013-Ohio-3416.pdf\"\n",
    "# url = \"https://example.com/pdf_file.pdf\"\n",
    "# case_no = \"12345\"\n",
    "\n",
    "# Extract text content and prepare metadata\n",
    "# text_content = extract_text_from_pdf(pdf_path)\n",
    "# metadata = {\"url\": url, \"case_no\": case_no}\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=100,\n",
    "# )\n",
    "\n",
    "# docs = text_splitter.split_documents(text_content)\n",
    "# # print(docs)\n",
    "# len(docs)\n",
    "# # Create document with content, metadata, and required fields\n",
    "# document = {\n",
    "#     'id': 'pdf_id',  # Add a unique ID for the document\n",
    "#     'values': text_content,  # Assign the text content to the 'values' field\n",
    "#     'metadata': metadata  # Add metadata\n",
    "# }\n",
    "# # # print(text_content)\n",
    "# # Upsert the document into Pinecone DB\n",
    "# pinecone.Index(index_name).upsert([document])\n",
    "\n",
    "# # Disconnect from Pinecone\n",
    "# pinecone.deinit()\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=100,\n",
    "# )\n",
    "\n",
    "# # len(docs)\n",
    "# print(type(text_content))\n",
    "# docs = text_splitter.split_documents(text_content)\n",
    "# print(docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF FILE EXAMPLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "type(loader)\n",
    "file_content = loader.load()\n",
    "print(len(file_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(docs)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(docs[1])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(type(docs))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(docs)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#for every item in docs, replace '\\n' with ' '\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(file_content)\n",
    "# print(docs)\n",
    "len(docs)\n",
    "# print(docs[1])\n",
    "# print(type(docs))\n",
    "# print(docs)\n",
    "\n",
    "#for every item in docs, replace '\\n' with ' '\n",
    "for item in docs['Document']:\n",
    "    item.replace('\\n', ' ')\n",
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ad the url and case_no to the docs list\n",
    "# docs.append(url)\n",
    "# docs.append(case_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m meta_data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_no\u001b[39m\u001b[38;5;124m'\u001b[39m: case_no} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs))]\n\u001b[1;32m      2\u001b[0m docs_with_metadata \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: doc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m: meta} \u001b[38;5;28;01mfor\u001b[39;00m doc, meta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(docs, meta_data)]\n\u001b[0;32m----> 4\u001b[0m docsearch \u001b[38;5;241m=\u001b[39m \u001b[43mPinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs_with_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ohio_case_scrape/.venv/lib/python3.11/site-packages/langchain_core/vectorstores.py:508\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[VST],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     texts \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    509\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/ohio_case_scrape/.venv/lib/python3.11/site-packages/langchain_core/vectorstores.py:508\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[VST],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    509\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "meta_data = [{'url': url, 'case_no': case_no} for _ in range(len(docs))]\n",
    "docs_with_metadata = [{'content': doc, 'meta': meta} for doc, meta in zip(docs, meta_data)]\n",
    "\n",
    "docsearch = Pinecone.from_documents(docs_with_metadata, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "query = \"What company is this document from?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What does the document say about privacy rights?\"\n",
    "resp1 = chain.run(input_documents=docs, question=query1)\n",
    "resp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Create a summary of the key facts of the case. If a civil case this is an example, 'John Smith on 12/12/2019 went into the store and fell on a piece of fruit.  He injured his back and later had surgery.  He then filed a lawsuit against the store for negligence.  At trial, the store prevailed and John appealed resulting in this decision.'  If a criminal case, as part of the facts, ensure you list the crimes the defendant was charged with and the trial court's determination of those charges.  Display the facts in a list format one per line.  After the list of facts, determine what the reason was for the appeal.  Finally, for the last fact, summarize the decision of the court.\"\n",
    "\n",
    "query2 = \"Generate a summary of the legal issues involved in the case.  List the statutes, rules or regulations that were being argued about in the case.  Display the legal issues in a list format one per line.  After the list of legal issues, determine what the reason was for the appeal.  Finally, for the last legal issue, summarize the decision of the court.\"\n",
    "\n",
    "query3 = \"Generate a list of all the parties involved in the case.  Display the parties in a list format one per line.  After the list of parties, determine what the reason was for the appeal.  Finally, for the last party, summarize the decision of the court.\"\n",
    "\n",
    "query4 = \"Create a list of the procedural history of the case.  Display the procedural history in a list format one per line.  After the list of procedural history, determine what the reason was for the appeal.  Finally, for the last procedural history, summarize the decision of the court.\"\n",
    "# query2 = \"Who are the judges who decided the case?\"\n",
    "# query3 = \"What date was this decision published?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp1 = chain.run(input_documents=docs, question=query1)\n",
    "resp2 = chain.run(input_documents=docs, question=query2)\n",
    "resp3 = chain.run(input_documents=docs, question=query3)\n",
    "resp4 = chain.run(input_documents=docs, question=query4)\n",
    "print(f\"{resp1}\")\n",
    "print(f\"{resp2}\")\n",
    "print(f\"{resp3}\")\n",
    "print(f\"{resp4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = \"What is the appellate case number?\"\n",
    "query5 = \"which Judge authored the decision?\"\n",
    "query6 = \"What are the basic facts of this case?\"\n",
    "\n",
    "\n",
    "resp4 = chain.run(input_documents=docs, question=query4)\n",
    "resp5 = chain.run(input_documents=docs, question=query5)\n",
    "resp6 = chain.run(input_documents=docs, question=query6)\n",
    "\n",
    "print(f\"{query4}: {resp4}\")\n",
    "print(f\"{query5}: {resp5}\")\n",
    "print(f\"{query6}: {resp6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query7 = \"Was this a criminal or civil case?\"\n",
    "\n",
    "resp7 = chain.run(input_documents=docs, question=query7)\n",
    "\n",
    "print(f\"{query7}: {resp7}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_texts = text_splitter.split_documents(file_content)\n",
    "print(len(case_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_docsearch = Pinecone.from_documents(case_texts, embeddings, index_name=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERYING THE PDF DOCUMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who are the parties to this case?\"\n",
    "docs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(file_content))\n",
    "print(len(file_content))\n",
    "# file_content[10]\n",
    "pinecone.init(      \n",
    "\tapi_key=PINECONE_API_KEY,      \n",
    "\tenvironment=PINECONE_ENV     \n",
    ")      \n",
    "index_name = pinecone.Index('embedder-index')\n",
    "# # The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\n",
    "# docsearch = Pinecone.from_documents(docs, embeddings, index_name=index)\n",
    "active_indexes = pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=20\n",
    ")\n",
    "texts = text_splitter.split_text(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=20\n",
    ")\n",
    "case_texts = text_splitter.split_text(file_content)\n",
    "print(len(case_texts))\n",
    "type(case_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_docsearch = Pinecone.from_texts(file_content, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITTING THE TEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = open('Oz_book.txt', 'r').read()\n",
    "# len(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=20\n",
    ")\n",
    "book_texts = text_splitter.split_text(file_content)\n",
    "print(len(book_texts))\n",
    "type(book_texts)\n",
    "# print(book_texts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PINECONE AND OPENAI EMBEDDING SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key = PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENV\n",
    ")\n",
    "index_name = 'embedder-index'\n",
    "case_docsearch=Pinecone.from_texts((t.page_content for t in book_texts), index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
